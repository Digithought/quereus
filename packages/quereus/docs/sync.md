# Sync Module - Multi-Master CRDT Replication

This document describes the architecture for `quereus-plugin-sync`, a fully automatic multi-master CRDT replication system for Quereus. It enables offline-first applications where multiple replicas can independently modify data and converge to a consistent state.

## Design Goals

- **Fully Automatic**: All tables in the store are automatically CRDT-enabled. No opt-in required.
- **Automatic Schema Evolution**: Schema changes are tracked and synchronized without special handling.
- **Transport Agnostic**: Exposes sync data structures and APIs without assuming any transport layer.
- **Backend Agnostic**: Works with both LevelDB (Node.js) and IndexedDB (browser) via the store plugin.
- **Reactive**: Exposes hooks for UI reactivity when data changes from local or remote sources.
- **Transaction-Aware**: Changes are grouped by transaction for atomic sync operations.

## Architecture Overview

```
┌────────────────────────────────────────────────────────────────────────────┐
│                         Application Layer                                   │
│  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────────────────────┐ │
│  │   Quereus   │  │ Sync Hooks  │  │     Transport (user-provided)       │ │
│  │  Database   │  │ (reactive)  │  │  WebSocket / HTTP / WebRTC / etc.   │ │
│  └──────┬──────┘  └──────┬──────┘  └─────────────────┬───────────────────┘ │
│         │                │                           │                      │
├─────────┼────────────────┼───────────────────────────┼──────────────────────┤
│         ▼                ▼                           ▼                      │
│  ┌──────────────────────────────────────────────────────────────────────┐  │
│  │                      quereus-plugin-sync                              │  │
│  │  ┌────────────┐  ┌────────────┐  ┌────────────┐  ┌────────────────┐  │  │
│  │  │    HLC     │  │  Metadata  │  │   Sync     │  │    Schema      │  │  │
│  │  │   Clock    │  │   Store    │  │  Protocol  │  │   Tracker      │  │  │
│  │  └────────────┘  └────────────┘  └────────────┘  └────────────────┘  │  │
│  │                                                                       │  │
│  │  ┌────────────────────────────────────────────────────────────────┐  │  │
│  │  │                    SyncModule (wrapper)                         │  │  │
│  │  │  Intercepts mutations → Records CRDT metadata → Delegates       │  │  │
│  │  └────────────────────────────────────────────────────────────────┘  │  │
│  └──────────────────────────────────────────────────────────────────────┘  │
│                                    │                                        │
├────────────────────────────────────┼────────────────────────────────────────┤
│                                    ▼                                        │
│  ┌──────────────────────────────────────────────────────────────────────┐  │
│  │                      quereus-store                                   │  │
│  │  ┌─────────────────────────┐  ┌─────────────────────────┐            │  │
│  │  │   LevelDB (Node.js)     │  │   IndexedDB (Browser)   │            │  │
│  │  │   Data + CRDT Metadata  │  │   Data + CRDT Metadata  │            │  │
│  │  └─────────────────────────┘  └─────────────────────────┘            │  │
│  └──────────────────────────────────────────────────────────────────────┘  │
└────────────────────────────────────────────────────────────────────────────┘
```

## Core Concepts

### Hybrid Logical Clock (HLC)

The sync module uses a Hybrid Logical Clock to establish causal ordering of events across distributed replicas. HLC combines:

- **Physical Time**: Wall clock time in milliseconds for rough ordering
- **Logical Counter**: Disambiguates events within the same millisecond
- **Site ID**: 16-byte UUID identifying each replica

```typescript
interface HLC {
  wallTime: bigint;      // Physical time (ms since epoch)
  counter: number;       // Logical counter (0-65535)
  siteId: Uint8Array;    // 16-byte replica UUID
}
```

HLC ordering: `(wallTime, counter, siteId)` compared lexicographically. This ensures:
- Events with higher wall time are considered newer
- Events at the same wall time are ordered by counter
- Ties are broken deterministically by site ID

### Conflict Resolution: Column-Level Last-Write-Wins (LWW)

Each column of each row is tracked independently. When the same column is modified on multiple replicas, the write with the highest HLC wins.

```
Replica A: UPDATE users SET name = 'Alice' WHERE id = 1  @ HLC(1000, 1, A)
Replica B: UPDATE users SET email = 'b@x.com' WHERE id = 1  @ HLC(1000, 2, B)

After merge: Row has name='Alice' (from A) AND email='b@x.com' (from B)
```

This is more fine-grained than row-level LWW, preserving more user intent.

> **Future Work**: The architecture supports extending to other CRDT types (counters, sets, RGA for text) by tracking different metadata per column type.

### Tombstones and Deletions

Deletions are recorded as "tombstones" with an HLC timestamp. Tombstones prevent deleted rows from being resurrected by older writes that arrive later.

**Resurrection Policy** (configurable):
- **Default: Delete Wins** - A deletion with HLC(T1) prevents any column write with HLC < T1
- **Optional: Resurrection Allowed** - An insert/update with HLC > T1 can resurrect a deleted row

**Tombstone TTL**: Tombstones are retained for a configurable duration (default: 30 days). Sync attempts after TTL expiration should fall back to full snapshot transfer.

### Transaction-Based Change Grouping

Changes are grouped by transaction. When syncing:
- All changes within a transaction are sent as a unit
- Applying changes is atomic per transaction
- This preserves referential integrity across related writes

### Transactional Integrity During Sync

When applying remote changes, the sync system must write to two separate stores:
1. **CRDT metadata** → sync metadata store (column versions, tombstones, peer state)
2. **Actual table data** → each table's data store

**Challenge**: In IndexedDB, each table has its own database, so we cannot have a single atomic transaction spanning both the metadata store and multiple table stores. LevelDB uses a single database with key prefixes, allowing atomic `WriteBatch` commits across tables.

**Write Order**: To ensure crash safety, changes must be applied in this order:
1. **Data first**: Write table data to the data store
2. **Metadata second**: Write CRDT metadata to the sync store

This order is safe because:
- If crash occurs before data: nothing written, re-sync will retry
- If crash occurs after data but before metadata: CRDT state is "dirty" and will re-apply the same changes on next sync. Since CRDT operations are idempotent (same HLC → same LWW outcome), re-applying is safe.
- If crash occurs after metadata: all writes complete, consistent state

The reverse order (metadata first) would be dangerous: if we crash after writing metadata but before data, the CRDT state believes the change is applied but data is missing—and re-sync won't retry.

**Current Status**: ⚠️ The current implementation writes metadata first, then data. This should be reversed.

**Per-Table Batching**: Within each table, changes should be applied using `WriteBatch` for atomicity. The `TransactionCoordinator` in the Store module provides this capability.

**Atomicity Gap (IndexedDB)**: The legacy `IndexedDBModule` uses separate databases per table. The new `UnifiedIndexedDBModule` (Store Phase 7) solves this by placing all tables in a single database with object stores, enabling atomic cross-table transactions via `MultiStoreWriteBatch`.

**Isolation Gap**: Even with correct write ordering, readers may see partially-applied state during sync. True isolation would require Store-level support—see [Future: Store Isolation](#future-store-isolation) below.

### Single-Database Architecture (Store Phase 7) ✓

The `UnifiedIndexedDBModule` uses a single IndexedDB database with multiple object stores (one per table). This enables atomic cross-table transactions.

| UnifiedIndexedDBModule | Legacy IndexedDBModule |
|------------------------|------------------------|
| ✅ Native cross-table IDB transactions | ❌ No cross-DB transactions |
| ✅ Sync metadata + data in one transaction | ❌ Sequential commits |
| ✅ No WAL needed for crash recovery | ⚠️ Would need WAL |
| ✅ Same storage quota | ✅ Same storage quota |

With `UnifiedIndexedDBModule`, sync can use `MultiStoreWriteBatch`:
```typescript
const batch = module.createMultiStoreBatch();
batch.putToStore('main.users', userKey, userData);
batch.putToStore('main.orders', orderKey, orderData);
batch.putToStore('__catalog__', metaKey, syncMetadata);
await batch.write();  // Native atomicity across all stores
```

### Store Isolation (Store Phase 8 - Future)

Longer-term, the Store module should provide transaction isolation similar to the memory vtab's layered architecture:

1. **TransactionLayer pattern**: Writers work on an isolated layer; readers see committed snapshot
2. **Copy-on-write semantics**: Inherited from memory vtab's BTree layering
3. **Atomic visibility**: All changes become visible at once on commit

If Store provides this primitive, sync can leverage it:
```
store.beginTransaction()    // Isolated write context
// Apply all data changes   (invisible to readers)
// Apply all CRDT metadata  (invisible to readers)
store.commit()              // Atomically visible
```

This would eliminate the isolation gap, providing true ACID semantics for sync operations across multiple tables. This is tracked in store.md as Phase 8.

## Storage Layout

CRDT metadata is stored alongside data in the same KV store using distinct key prefixes:

| Prefix | Purpose | Format |
|--------|---------|--------|
| `cv:{schema}.{table}:{pk}:{col}` | Column version | `{hlc, value}` |
| `tb:{schema}.{table}:{pk}` | Tombstone | `{hlc}` |
| `tx:{txId}` | Transaction record | `{changes[], hlc, committed}` |
| `ps:{siteId}` | Peer sync state | `{lastSyncHlc}` |
| `sm:{schema}.{table}:{version}` | Schema migration | `{ddl, hlc}` |
| `si:` | Site identity | `{siteId, createdAt}` |
| `hc:` | HLC state | `{wallTime, counter}` |

This co-location ensures:
- Atomic updates of data and metadata within transactions
- Single storage backend for both LevelDB and IndexedDB
- No additional database connections needed

## Sync Protocol

### Data Structures

```typescript
/** Identifies a specific replica in the network */
type SiteId = Uint8Array;  // 16-byte UUID

/** A transaction's worth of changes */
interface ChangeSet {
  siteId: SiteId;                    // Origin replica
  transactionId: string;             // Unique transaction ID
  hlc: HLC;                          // Transaction commit time
  changes: Change[];                 // Column-level changes
  schemaMigrations: SchemaMigration[]; // Schema changes in this tx
}

/** A single column modification */
interface ColumnChange {
  type: 'column';
  schema: string;
  table: string;
  pk: SqlValue[];                    // Primary key values
  column: string;
  value: SqlValue;
  hlc: HLC;
}

/** A row deletion */
interface RowDeletion {
  type: 'delete';
  schema: string;
  table: string;
  pk: SqlValue[];
  hlc: HLC;
}

type Change = ColumnChange | RowDeletion;

/** A schema modification */
interface SchemaMigration {
  type: 'create_table' | 'drop_table' | 'add_column' | 'drop_column' | 'add_index' | 'drop_index';
  schema: string;
  table: string;
  ddl: string;                       // The DDL statement
  hlc: HLC;
  schemaVersion: number;             // Monotonic per-table version
}
```

### Sync API

```typescript
interface SyncManager {
  /** Get this replica's site ID */
  getSiteId(): SiteId;

  /** Get current HLC for state comparison */
  getCurrentHLC(): HLC;

  /**
   * Get all changes since a peer's last known state.
   * For initial sync, omit sinceHLC to get full snapshot.
   */
  getChangesSince(peerSiteId: SiteId, sinceHLC?: HLC): Promise<ChangeSet[]>;

  /**
   * Apply changes received from a peer.
   * Returns statistics about what was applied.
   */
  applyChanges(changes: ChangeSet[]): Promise<ApplyResult>;

  /**
   * Check if delta sync is possible or if snapshot is required.
   * Returns false if tombstone TTL has expired for relevant data.
   */
  canDeltaSync(peerSiteId: SiteId, sinceHLC: HLC): Promise<boolean>;

  /**
   * Get a full snapshot for initial sync or TTL expiration recovery.
   */
  getSnapshot(): Promise<Snapshot>;

  /**
   * Apply a full snapshot (replaces all local data).
   */
  applySnapshot(snapshot: Snapshot): Promise<void>;
}

interface ApplyResult {
  applied: number;      // Changes successfully applied
  skipped: number;      // Changes already present (no-op due to LWW)
  conflicts: number;    // Conflicts resolved (remote won or lost)
  transactions: number; // Number of transactions processed
}

interface Snapshot {
  siteId: SiteId;
  hlc: HLC;
  tables: TableSnapshot[];
  schema: SchemaMigration[];
}

interface TableSnapshot {
  schema: string;
  table: string;
  rows: Row[];
  columnVersions: Map<string, HLC>;  // Per-column HLC for each row
}

// ============================================================================
// Streaming Snapshot API (for large datasets)
// ============================================================================

interface SyncManager {
  // ... existing methods ...

  /**
   * Stream a snapshot as chunks for memory-efficient transfer.
   * Use this instead of getSnapshot() for large databases.
   */
  getSnapshotStream(chunkSize?: number): AsyncIterable<SnapshotChunk>;

  /**
   * Apply a streamed snapshot with progress tracking.
   * Supports resumption via checkpoint tracking.
   */
  applySnapshotStream(
    chunks: AsyncIterable<SnapshotChunk>,
    onProgress?: (progress: SnapshotProgress) => void
  ): Promise<void>;

  /**
   * Get a resumable checkpoint for an in-progress snapshot.
   */
  getSnapshotCheckpoint(snapshotId: string): Promise<SnapshotCheckpoint | undefined>;

  /**
   * Resume a snapshot transfer from a checkpoint.
   */
  resumeSnapshotStream(checkpoint: SnapshotCheckpoint): AsyncIterable<SnapshotChunk>;
}

/** Snapshot chunk types for streaming */
type SnapshotChunk =
  | SnapshotHeaderChunk      // Sent first with metadata
  | SnapshotTableStartChunk  // Marks beginning of a table
  | SnapshotColumnVersionsChunk  // Batch of column versions
  | SnapshotTableEndChunk    // Marks end of a table
  | SnapshotSchemaMigrationChunk  // Schema migration
  | SnapshotFooterChunk;     // Sent last with stats

/** Progress info during snapshot streaming */
interface SnapshotProgress {
  snapshotId: string;
  tablesProcessed: number;
  totalTables: number;
  entriesProcessed: number;
  totalEntries: number;
  currentTable?: string;
}

/** Checkpoint for resumable snapshot transfers */
interface SnapshotCheckpoint {
  snapshotId: string;
  siteId: SiteId;
  hlc: HLC;
  lastTableIndex: number;
  lastEntryIndex: number;
  completedTables: string[];
  entriesProcessed: number;
  createdAt: number;
}
```

### Sync Flow (Master to Many-Masters)

For the primary use case of a master server syncing to many frontend replicas:

```
┌─────────────┐                              ┌─────────────┐
│   Master    │                              │  Frontend   │
│   Server    │                              │  Replica    │
└──────┬──────┘                              └──────┬──────┘
       │                                            │
       │  1. Frontend connects, sends:              │
       │     { mySiteId, lastSyncHLC }              │
       │◄───────────────────────────────────────────│
       │                                            │
       │  2. Master checks canDeltaSync()           │
       │     If false: send full snapshot           │
       │     If true: getChangesSince()             │
       │                                            │
       │  3. Master sends ChangeSet[]               │
       │────────────────────────────────────────────►
       │                                            │
       │  4. Frontend applies changes               │
       │     applyChanges(changeSets)               │
       │                                            │
       │  5. Frontend sends its local changes       │
       │     (changes made while offline)           │
       │◄───────────────────────────────────────────│
       │                                            │
       │  6. Master applies frontend changes        │
       │     Conflicts resolved via LWW             │
       │                                            │
       │  7. If conflicts, master re-sends winners  │
       │────────────────────────────────────────────►
       │                                            │
```

### WebSocket Sync Protocol

The WebSocket protocol provides real-time bidirectional synchronization. This is the recommended transport for interactive applications.

```
┌────────────────────────────────────────────────────────────────────────────┐
│                        WebSocket Message Flow                               │
├────────────────────────────────────────────────────────────────────────────┤
│                                                                            │
│  CLIENT                                            SERVER                  │
│    │                                                  │                    │
│    │──────── { type: "handshake", siteId, token? } ──►│                    │
│    │                                                  │                    │
│    │◄─────── { type: "handshake_ack", serverSiteId } ─│                    │
│    │                                                  │                    │
│    │──────── { type: "get_changes", sinceHLC? } ─────►│                    │
│    │                                                  │                    │
│    │◄─────── { type: "changes", changeSets: [...] } ──│                    │
│    │                                                  │                    │
│    │──────── { type: "apply_changes", changes } ─────►│  (local changes)   │
│    │                                                  │                    │
│    │◄─────── { type: "apply_result", applied, ... } ──│                    │
│    │                                                  │                    │
│    │◄────── { type: "push_changes", changeSets } ─────│  (from other peer) │
│    │                                                  │                    │
│    │──────── { type: "ping" } ───────────────────────►│  (heartbeat)       │
│    │◄─────── { type: "pong" } ────────────────────────│                    │
│    │                                                  │                    │
└────────────────────────────────────────────────────────────────────────────┘
```

#### Message Types

**Client → Server:**

| Type | Purpose | Payload |
|------|---------|---------|
| `handshake` | Authenticate and establish session | `{ siteId, token? }` |
| `get_changes` | Request changes since an HLC | `{ sinceHLC? }` (base64) |
| `apply_changes` | Push local changes to server | `{ changes: ChangeSet[] }` |
| `get_snapshot` | Request full snapshot | (none) |
| `ping` | Heartbeat / keepalive | (none) |

**Server → Client:**

| Type | Purpose | Payload |
|------|---------|---------|
| `handshake_ack` | Confirm authentication | `{ serverSiteId, connectionId }` |
| `changes` | Response to `get_changes` | `{ changeSets: ChangeSet[] }` |
| `push_changes` | Broadcast from another client | `{ changeSets: ChangeSet[] }` |
| `apply_result` | Confirm changes applied | `{ applied, skipped, conflicts }` |
| `snapshot_chunk` | Streamed snapshot data | `{ ...SnapshotChunk }` |
| `error` | Error response | `{ code, message }` |
| `pong` | Heartbeat response | (none) |

#### Connection Lifecycle

```
┌────────────────────────────────────────────────────────────────────────────┐
│                        Client Connection State Machine                      │
├────────────────────────────────────────────────────────────────────────────┤
│                                                                            │
│   ┌─────────────┐                                                          │
│   │ DISCONNECTED│◄─────────────────────────────────────────────┐           │
│   └──────┬──────┘                                              │           │
│          │ connectSync(url, token)                             │           │
│          ▼                                                     │           │
│   ┌─────────────┐                                              │           │
│   │ CONNECTING  │──────────────────────────────────────────────┤           │
│   └──────┬──────┘  WebSocket error or close                    │           │
│          │ onopen → send handshake                             │           │
│          ▼                                                     │           │
│   ┌─────────────┐                                              │           │
│   │   SYNCING   │──────────────────────────────────────────────┤           │
│   └──────┬──────┘  handshake_ack → get_changes                 │           │
│          │ changes received → applyChanges()                   │           │
│          ▼                                                     │           │
│   ┌─────────────┐                                              │           │
│   │   SYNCED    │◄─────┐                                       │           │
│   └──────┬──────┘      │ apply_result or push_changes applied  │           │
│          │             │                                       │           │
│          └─────────────┘ local change → apply_changes          │           │
│          │                                                     │           │
│          │ WebSocket close (unintentional)                     │           │
│          ▼                                                     │           │
│   ┌─────────────┐                                              │           │
│   │ RECONNECTING│─── exponential backoff (1s, 2s, 4s... 60s) ──┘           │
│   └─────────────┘                                                          │
│                                                                            │
└────────────────────────────────────────────────────────────────────────────┘
```

#### Delta Sync Optimization

To minimize data transfer, clients track sync progress with the server:

1. **Receiving changes**: After applying server changes, client updates `peerSyncState[serverSiteId]` with the max HLC received
2. **Sending changes**: Client tracks `lastSentHLC` (confirmed) and `pendingSentHLC` (awaiting ack)
3. **Reconnection**: On reconnect, client sends `get_changes` with `sinceHLC` from peer sync state
4. **Server tracking**: Server uses client's `sinceHLC` to return only new changes

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        Delta Sync State Tracking                            │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Client State:                     Server State:                            │
│  ┌─────────────────────────────┐   ┌─────────────────────────────┐          │
│  │ peerSyncState[serverSiteId] │   │ Change Log (HLC-indexed)    │          │
│  │   └─ lastReceivedHLC        │   │   └─ All changes since T0   │          │
│  │                             │   │                             │          │
│  │ lastSentHLC (confirmed)     │   │ Per-client session:         │          │
│  │ pendingSentHLC (in-flight)  │   │   └─ lastSyncHLC            │          │
│  └─────────────────────────────┘   └─────────────────────────────┘          │
│                                                                             │
│  On reconnect:                                                              │
│  1. Client: get_changes { sinceHLC: peerSyncState[serverSiteId] }           │
│  2. Server: Returns only changes where change.hlc > sinceHLC                │
│  3. Client: applyChanges(), updates peerSyncState                           │
│                                                                             │
│  On local change:                                                           │
│  1. Local change triggers debounced send (50ms window)                      │
│  2. Client: getChangesSince(serverSiteId, lastSentHLC) → filtered changes   │
│  3. Client: apply_changes { changes }                                       │
│  4. Client: pendingSentHLC = max HLC of sent changes                        │
│  5. Server: apply_result { applied, ... }                                   │
│  6. Client: lastSentHLC = pendingSentHLC (on success)                       │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

#### Reconnection with Exponential Backoff

When the WebSocket connection drops unexpectedly:

1. Client schedules reconnect with exponential backoff: `delay = min(1s × 2^attempt, 60s)`
2. On successful reconnect, attempt counter resets to 0
3. Manual `disconnectSync()` sets `intentionalDisconnect = true` to prevent auto-reconnect
4. Reconnect attempts use the same URL and token from the original connection

#### Local Change Debouncing

Rapid local changes are batched to reduce network overhead:

1. On local change event, start/reset a 50ms debounce timer
2. When timer fires, collect all changes since `lastSentHLC`
3. Send batched changes in a single `apply_changes` message
4. This reduces WebSocket messages from N per edit to 1 per burst

## Reactive Hooks

The sync module exposes reactive hooks for UI integration:

```typescript
interface SyncEventEmitter {
  /** Fired when remote changes are applied locally */
  onRemoteChange(listener: (event: RemoteChangeEvent) => void): () => void;

  /** Fired when local changes are ready to sync */
  onLocalChange(listener: (event: LocalChangeEvent) => void): () => void;

  /** Fired when sync state changes (connected, syncing, error) */
  onSyncStateChange(listener: (state: SyncState) => void): () => void;

  /** Fired when a conflict is resolved */
  onConflictResolved(listener: (event: ConflictEvent) => void): () => void;
}

interface RemoteChangeEvent {
  siteId: SiteId;                    // Origin replica
  transactionId: string;
  changes: Change[];
  appliedAt: HLC;
}

interface LocalChangeEvent {
  transactionId: string;
  changes: Change[];
  pendingSync: boolean;              // True if not yet synced to master
}

interface ConflictEvent {
  table: string;
  pk: SqlValue[];
  column: string;
  localValue: SqlValue;
  remoteValue: SqlValue;
  winner: 'local' | 'remote';
  winningHLC: HLC;
}

type SyncState =
  | { status: 'disconnected' }
  | { status: 'connecting' }
  | { status: 'syncing'; progress: number }
  | { status: 'synced'; lastSyncHLC: HLC }
  | { status: 'error'; error: Error };
```

### Integration with Store Events

The sync module subscribes to the store's `StoreEventEmitter` to capture mutations. A key design goal is that reactive events fire **exactly once** for each change, whether the change is local or remote.

#### Event Flow

**Local Changes:**
```
User SQL → Store executes → Store emits event (remote=false) → SyncManager records metadata → UI receives event
```

**Remote Changes:**
```
SyncManager receives remote change → Updates metadata → Calls applyToStore → Store executes → Store emits event (remote=true) → SyncManager ignores → UI receives event
```

In both cases, the UI receives exactly one event from the Store. The `remote` flag determines whether the SyncManager should record CRDT metadata (local) or skip (remote).

#### The `remote` Flag

Both `DataChangeEvent` and `SchemaChangeEvent` include a `remote?: boolean` flag:

```typescript
interface DataChangeEvent {
  type: 'insert' | 'update' | 'delete';
  schemaName: string;
  tableName: string;
  key: SqlValue[];
  oldRow?: Row;
  newRow?: Row;
  remote?: boolean;  // True if from sync or cross-tab
}

interface SchemaChangeEvent {
  type: 'create' | 'alter' | 'drop';
  objectType: 'table' | 'index' | 'view' | 'trigger';
  schemaName: string;
  objectName: string;
  ddl?: string;
  remote?: boolean;  // True if from sync
}
```

#### Sync Module Event Handling

```typescript
// Sync module listens to store events
storeEventEmitter.onDataChange((event) => {
  // Skip events from remote sync - metadata already recorded
  if (event.remote) return;

  // Record CRDT metadata for local changes only
  syncModule.recordChange(event);

  // Emit for UI reactivity (local change pending sync)
  syncEventEmitter.emitLocalChange({
    transactionId: currentTxId,
    changes: [eventToChange(event)],
    pendingSync: true,
  });
});

storeEventEmitter.onSchemaChange((event) => {
  // Skip events from remote sync - metadata already recorded
  if (event.remote) return;

  // Record schema CRDT metadata for local changes
  syncModule.recordSchemaChange(event);
});
```

#### Applying Remote Changes

When the SyncManager applies remote changes, it must execute SQL in a way that the resulting store events are marked with `remote: true`:

```typescript
// SyncManager applies a remote changeset
async applyRemoteChangeset(changeset: ChangeSet): Promise<void> {
  // 1. Update CRDT metadata first (before SQL execution)
  for (const change of changeset.changes) {
    await this.updateMetadataForRemote(change);
  }

  // 2. Apply to store with remote flag
  await this.applyToStore(changeset.changes, { remote: true });
  // Store emits events with remote=true, SyncManager ignores them
}
```

The store plugin provides a mechanism to execute SQL with the remote flag:

```typescript
interface ApplyOptions {
  remote?: boolean;  // Mark resulting events as remote
}

// Store implementation ensures emitted events have remote=true
async applyChanges(changes: Change[], options: ApplyOptions): Promise<void> {
  for (const change of changes) {
    // Execute SQL...
    // When emitting event, include remote flag from options
    this.events.emitDataChange({ ...event, remote: options.remote });
  }
}
```

## Schema Synchronization

Schema (catalog) changes are synchronized using the same CRDT approach as data, ensuring eventual convergence across all replicas without requiring a perpetual migration log.

### Design Principles

1. **Catalog as Data**: Schema elements (tables, columns, indexes) are tracked with HLCs just like row data
2. **Column-Level Granularity**: Each column definition has its own HLC, enabling parallel schema changes
3. **Most Destructive Wins**: DROP operations take precedence over modifications
4. **DDL Before DML**: Sync batches always apply schema changes before data changes
5. **No Perpetual Log**: Only current state is tracked, not a history of migrations

### Schema Metadata Storage

Schema metadata is stored alongside data metadata using the same patterns:

| Key Pattern | Purpose | Value |
|-------------|---------|-------|
| `sv:{schema}.{table}:__table__` | Table existence | `{hlc, exists, ddl}` |
| `sv:{schema}.{table}:{column}` | Column definition | `{hlc, definition, deleted?}` |
| `sv:{schema}.{table}:{index}:__index__` | Index definition | `{hlc, definition, deleted?}` |

### Conflict Resolution: Most Destructive Wins

Schema conflicts follow a hierarchy where more destructive operations take precedence:

```
DROP TABLE > DROP COLUMN > ALTER COLUMN > ADD COLUMN
DROP TABLE > DROP INDEX > CREATE INDEX
```

Within the same level of destructiveness, Last-Write-Wins (LWW) applies based on HLC.

**Examples:**

```
Replica A: DROP COLUMN foo      @ HLC(1000, 1, A)
Replica B: ALTER COLUMN foo...  @ HLC(2000, 1, B)

Resolution: DROP wins (more destructive), even though B has higher HLC.
```

```
Replica A: ALTER COLUMN foo SET DEFAULT 'x'  @ HLC(1000, 1, A)
Replica B: ALTER COLUMN foo SET DEFAULT 'y'  @ HLC(2000, 1, B)

Resolution: B wins (same level, higher HLC).
```

```
Replica A: ADD COLUMN bar INTEGER  @ HLC(1000, 1, A)
Replica B: ADD COLUMN bar TEXT     @ HLC(2000, 1, B)

Resolution: B wins (same level, higher HLC). Column ends up as TEXT.
```

### DDL Application Order

When applying a sync batch:

1. **Schema changes first**: All DDL operations are applied before any DML
2. **Destructive operations first**: DROP TABLE, then DROP COLUMN, then ALTER/ADD
3. **Data changes second**: INSERT/UPDATE/DELETE applied to the now-correct schema

This ensures that structures always exist before data referencing them arrives.

### Schema Change Types

```typescript
type SchemaChangeType =
  | 'create_table'
  | 'drop_table'
  | 'add_column'
  | 'drop_column'
  | 'alter_column'
  | 'create_index'
  | 'drop_index'
  | 'create_view'
  | 'drop_view'
  | 'create_trigger'
  | 'drop_trigger';

interface SchemaChange {
  type: SchemaChangeType;
  schema: string;
  table: string;
  column?: string;           // For column operations
  objectName?: string;       // For index/view/trigger
  definition?: string;       // DDL or column definition
  hlc: HLC;
  deleted?: boolean;         // True for DROP operations
}
```

### Applying Remote Schema Changes

When a remote schema change is received:

1. Compare HLCs using the "most destructive wins" rule
2. If remote wins, update local schema metadata
3. Execute the DDL against the database (with `remote: true` flag)
4. The store emits schema change events for UI reactivity

```typescript
async applySchemaChange(change: SchemaChange): Promise<'applied' | 'skipped'> {
  const local = await this.getSchemaVersion(change.schema, change.table, change.column);

  if (local && !this.shouldApplySchemaChange(change, local)) {
    return 'skipped';
  }

  // Update metadata
  await this.setSchemaVersion(change.schema, change.table, change.column, {
    hlc: change.hlc,
    definition: change.definition,
    deleted: change.deleted,
  });

  // Execute DDL via callback (store applies with remote flag)
  if (change.definition) {
    await this.applyDDL(change.definition, { remote: true });
  }

  return 'applied';
}

private shouldApplySchemaChange(remote: SchemaChange, local: SchemaVersion): boolean {
  // Most destructive wins
  if (remote.deleted && !local.deleted) return true;   // DROP beats non-DROP
  if (!remote.deleted && local.deleted) return false;  // non-DROP loses to DROP

  // Same level: LWW
  return compareHLC(remote.hlc, local.hlc) > 0;
}
```

## Configuration

```typescript
interface SyncConfig {
  /** Tombstone retention period in milliseconds (default: 30 days) */
  tombstoneTTL: number;

  /** Whether deleted rows can be resurrected by later writes (default: false) */
  allowResurrection: boolean;

  /** Maximum changes per sync batch (default: 1000) */
  batchSize: number;

  /** Site ID (auto-generated if not provided) */
  siteId?: Uint8Array;
}

// Usage
const sync = createSyncModule(storeModule, storeEventEmitter, {
  tombstoneTTL: 30 * 24 * 60 * 60 * 1000,  // 30 days
  allowResurrection: false,
  batchSize: 1000,
});
```

## Usage Example

```typescript
import { Database } from '@quereus/quereus';
import { LevelDBModule, LevelDBStore, StoreEventEmitter } from 'quereus-store';
import { createSyncModule } from 'quereus-plugin-sync';

// 1. Set up store with event emitter
const storeEvents = new StoreEventEmitter();
const store = new LevelDBModule(storeEvents);

// 2. Open a KV store for sync metadata
const kvStore = await LevelDBStore.open({ path: './sync-meta' });

// 3. Create sync module
const { syncManager, syncEvents } = await createSyncModule(kvStore, storeEvents, {
  tombstoneTTL: 30 * 24 * 60 * 60 * 1000,
});

// 4. Register store module with database
const db = new Database();
db.registerVtabModule('store', store);

// 5. Create tables (sync automatically tracks changes via storeEvents)
await db.exec(`
  create table users (
    id integer primary key,
    name text,
    email text
  ) using store(path='./data')
`);

// 6. Subscribe to sync events for UI
syncEvents.onRemoteChange((event) => {
  console.log('Remote changes applied:', event.changes.length);
  // Update UI, invalidate caches, etc.
});

syncEvents.onConflictResolved((event) => {
  console.log(`Conflict on ${event.table}.${event.column}: ${event.winner} won`);
});

// 7. Implement your transport layer
async function syncWithServer(ws: WebSocket) {
  // Get changes to send
  const localChanges = await syncManager.getChangesSince(
    serverSiteId,
    lastServerHLC
  );

  // Send via your transport
  ws.send(JSON.stringify({ type: 'changes', data: localChanges }));

  // Receive and apply server changes
  ws.onmessage = async (msg) => {
    const serverChanges = JSON.parse(msg.data);
    const result = await syncManager.applyChanges(serverChanges);
    console.log(`Applied ${result.applied} changes`);
  };
}
```

### Streaming Snapshot Example

For large databases, use streaming snapshots to avoid loading everything into memory:

```typescript
// Server: Stream snapshot to client
async function sendSnapshot(ws: WebSocket) {
  for await (const chunk of syncManager.getSnapshotStream(1000)) {
    ws.send(JSON.stringify(chunk));
  }
}

// Client: Apply streamed snapshot with progress
async function receiveSnapshot(ws: WebSocket) {
  const chunks = receiveChunks(ws); // Your async iterator over WebSocket messages

  await syncManager.applySnapshotStream(chunks, (progress) => {
    console.log(`Progress: ${progress.tablesProcessed}/${progress.totalTables} tables`);
    console.log(`Entries: ${progress.entriesProcessed}/${progress.totalEntries}`);
  });
}

// Resume interrupted snapshot
async function resumeSnapshot(ws: WebSocket) {
  const checkpoint = await syncManager.getSnapshotCheckpoint(snapshotId);
  if (checkpoint) {
    // Request resume from server
    ws.send(JSON.stringify({ type: 'resume', checkpoint }));

    // Server resumes from checkpoint
    for await (const chunk of syncManager.resumeSnapshotStream(checkpoint)) {
      ws.send(JSON.stringify(chunk));
    }
  }
}
```

### Store Adapter for Remote Changes

The `createStoreAdapter` function creates a unified adapter for applying remote changes to LevelDB and IndexedDB stores:

```typescript
import { createStoreAdapter } from 'quereus-plugin-sync';
import { LevelDBStore, StoreEventEmitter } from '@quereus/store';

// Create event emitter for store events
const storeEvents = new StoreEventEmitter();

// Open your KV store
const kvStore = await LevelDBStore.open({ path: './data' });

// Create the store adapter
const applyToStore = createStoreAdapter(kvStore, storeEvents);

// Use with SyncManager - remote changes are applied via the adapter
const syncManager = new SyncManagerImpl(metadataKvStore, storeEvents, applyToStore, {
  tombstoneTTL: 30 * 24 * 60 * 60 * 1000,
});

// When remote changes arrive, the adapter:
// 1. Handles UPSERT semantics (insert if row doesn't exist, update if it does)
// 2. Deletes rows by primary key
// 3. Executes DDL for schema changes
// 4. Emits events with remote=true to prevent re-recording CRDT metadata
```

## Implementation Status

### Completed

#### Phase 1: Core Infrastructure ✅
- [x] Create package structure (`quereus-plugin-sync`)
- [x] Implement HLC (Hybrid Logical Clock)
  - [x] `clock/hlc.ts` - HLC type, comparison, tick, receive
  - [x] `clock/site.ts` - Site ID generation and persistence
- [x] Implement CRDT metadata storage
  - [x] `metadata/keys.ts` - Key builders for sync metadata
  - [x] `metadata/column-version.ts` - Column version tracking
  - [x] `metadata/tombstones.ts` - Deletion tracking with TTL
  - [x] `metadata/peer-state.ts` - Peer sync state tracking
  - [x] `metadata/schema-migration.ts` - Schema change tracking

#### Phase 2: Sync Protocol ✅
- [x] Define protocol types (`sync/protocol.ts`)
- [x] Implement SyncManager interface (`sync/manager.ts`)
- [x] Implement SyncManagerImpl (`sync/sync-manager-impl.ts`)
  - [x] `applyChanges()` - Apply with LWW conflict resolution
  - [x] `canDeltaSync()` - TTL check for delta vs snapshot
  - [x] `updatePeerSyncState()` / `getPeerSyncState()` - Track peer sync progress

#### Phase 3: Event Integration ✅
- [x] Subscribe to `StoreEventEmitter` for data change events
- [x] Record column versions on insert/update
- [x] Record tombstones on deletion

#### Phase 4: Schema Sync ✅
- [x] `SchemaMigrationStore` - Track DDL changes with HLC
- [x] First-writer-wins conflict resolution for schema changes

#### Phase 5: Reactive Hooks ✅
- [x] Implement `SyncEventEmitter`
  - [x] `onRemoteChange` - Remote changes applied
  - [x] `onLocalChange` - Local changes pending
  - [x] `onSyncStateChange` - Connection state
  - [x] `onConflictResolved` - Conflict outcomes

#### Phase 6: Testing ✅
- [x] Unit tests for HLC
- [x] Unit tests for Site ID
- [x] Unit tests for ColumnVersionStore
- [x] Unit tests for TombstoneStore
- [x] Integration tests for SyncManager

#### Phase 7: Change Extraction ✅
- [x] `getChangesSince()` - Extract delta changes from metadata storage
- [x] `getSnapshot()` - Full snapshot for initial/recovery sync
- [x] `applySnapshot()` - Full state replacement
- [x] `pruneTombstones()` - Clean up expired tombstones

#### Phase 8: Streaming Snapshots ✅
- [x] `getSnapshotStream()` - Memory-efficient chunked snapshot streaming
- [x] `applySnapshotStream()` - Apply streamed snapshots with progress tracking
- [x] `getSnapshotCheckpoint()` / `resumeSnapshotStream()` - Resumable transfers
- [x] HLC-indexed change log for efficient delta queries

#### Phase 9: Remote Change Application ✅
- [x] `remote?: boolean` flag exists on both `DataChangeEvent` and `SchemaChangeEvent`
- [x] `handleDataChange()` skips events with `remote === true`
- [x] `handleSchemaChange()` skips events with `remote === true`
- [x] `applyToStore` callback mechanism for applying remote changes
  - [x] `ApplyToStoreCallback` type with `{ remote: true }` option
  - [x] `DataChangeToApply` / `SchemaChangeToApply` types for callback parameters
  - [x] Store implementations can emit events with `remote: true` flag
- [x] Reactive events fire exactly once (UI receives from Store, SyncManager ignores remote events)
- [x] Unit tests for `applyToStore` callback behavior

#### Phase 10: Store Integration ✅
- [x] Implement `createStoreAdapter()` - unified adapter for LevelDB and IndexedDB
- [x] Handle UPSERT semantics (column changes may be insert or update)
- [x] Handle row deletions by primary key
- [x] Execute DDL for schema changes with `remote: true`
- [x] Emit data change events with `remote: true` to prevent re-recording CRDT metadata

#### Phase 11: Schema Sync Refinement ✅
- [x] Implement column-level schema version storage (`SchemaVersionStore`)
- [x] Track schema elements with HLCs: `sv:{schema}.{table}:{column}` pattern
- [x] Implement "most destructive wins" conflict resolution
  - [x] `getDestructiveness()` - rank schema version types
  - [x] `getOperationDestructiveness()` - rank schema change operations
  - [x] `shouldApplySchemaChangeByOperation()` - compare changes with destructiveness hierarchy
- [x] Schema conflict tests (destructiveness ranking, LWW for same level)

#### Phase 12: Integration Testing ✅
- [x] E2E test: two replicas with bidirectional sync
- [x] Multi-replica conflict scenarios (concurrent writes to same column)
- [x] LWW conflict resolution tests
- [x] Delete-update conflict handling tests
- [x] Full snapshot sync between replicas

### Remaining Work

#### Transactional Integrity (Short-term)
- [ ] Fix write order in `applyChanges`: write data first, then CRDT metadata (see [Transactional Integrity During Sync](#transactional-integrity-during-sync))
- [ ] Use `WriteBatch` for per-table atomicity when applying remote changes
- [ ] Consider using `TransactionCoordinator` in store adapter for batched writes

#### Single-Database Architecture (Store Phase 7) ✓
- [x] Migrate IndexedDB to single database with multiple object stores (`UnifiedIndexedDBModule`)
- [x] Place sync metadata in same database as data tables (`__catalog__` object store)
- [x] Leverage native IDB transactions for cross-table atomicity (`MultiStoreWriteBatch`)
- [ ] Update sync store adapter to use `UnifiedIndexedDBModule` for atomic sync writes

#### Store Isolation (Longer-term - Store Phase 8)
- [ ] Implement isolation in Store module using memory vtab's TransactionLayer pattern
- [ ] Leverage Store isolation for sync to get true ACID semantics (see [Future: Store Isolation](#future-store-isolation))

#### Advanced Testing
- [ ] Tombstone TTL expiration and fallback to snapshot
- [ ] Large dataset streaming snapshot tests
- [ ] Network interruption / resume tests
- [ ] Integration tests with IndexedDB (browser environment)
- [ ] Crash recovery tests (verify idempotent re-apply after partial sync)

#### Documentation & Examples
- [ ] Example: WebSocket sync transport
- [ ] Example: HTTP polling sync transport
- [ ] Example: Implementing `applyToStore` callback
- [ ] Performance benchmarks

#### Reusable Sync Client Package (`@quereus/sync-client`) ✅

The WebSocket sync client is now available as a standalone package: [`@quereus/sync-client`](../../quereus-sync-client/).

**Features:**
- [x] WebSocket connection and handshake (`handshake` → `handshake_ack`)
- [x] Message dispatch (`changes`, `push_changes`, `apply_result`, `error`, `pong`)
- [x] ChangeSet serialization/deserialization (HLC, siteId encoding)
- [x] Local change debouncing (configurable, default 50ms)
- [x] Delta sync optimization (`lastSentHLC`, `pendingSentHLC` tracking)
- [x] Peer sync state tracking (`peerSyncState[serverSiteId]`)
- [x] Reconnection with exponential backoff (1s → 60s max)
- [x] Connection state machine (disconnected → connecting → syncing → synced)
- [x] Framework-agnostic design (no React/Svelte/Worker dependencies)

**`SyncClient` API:**
```typescript
import { SyncClient } from '@quereus/sync-client';

const client = new SyncClient({
  syncManager,
  syncEvents,                        // Local change listener
  onStatusChange: (status) => {},    // Connection state updates
  onRemoteChanges: (result, sets) => {}, // Applied remote changes
  onError: (error) => {},            // Error handling
  autoReconnect: true,               // Default: true
  reconnectDelayMs: 1000,            // Default: 1000
  maxReconnectDelayMs: 60000,        // Default: 60000
  localChangeDebounceMs: 50,         // Default: 50
});

await client.connect('wss://server/sync/ws', token);
// ... changes sync automatically ...
await client.disconnect();
```

**Completed:**
- [x] Create `packages/quereus-sync-client` package
- [x] Implement `SyncClient` class with WebSocket protocol
- [x] Extract serialization helpers
- [x] Add reconnection state machine with exponential backoff
- [x] Add delta sync tracking (peer sync state, sent HLC tracking)
- [x] Add local change listener with debouncing
- [x] Update `quoomb-web` worker to use `SyncClient`
- [x] Framework-agnostic design (no React/Svelte/Worker dependencies)

**Nice-to-have (future):**
- [ ] HTTP polling fallback for environments without WebSocket
- [ ] Connection quality metrics (latency, reconnect count)

---

## Schema Seed: App Provider as Sync Peer

This section describes how to distribute app schema migrations as a static "seed" that syncs into the user's database using the existing sync infrastructure. This pattern treats the app provider as a read-only peer with a well-known site ID.

### Motivation

When distributing an app with Quereus, the initial database schema (and optionally seed data) must be applied to each user's local database. Rather than using imperative migrations or version checks, we can leverage the CRDT sync infrastructure:

1. **Build time**: Generate a JSON bundle containing sync metadata for the app's schema
2. **Runtime**: Sync from the bundled seed into the user's database using `applyChanges()`
3. **Updates**: On app updates, only new schema changes are applied (delta sync)

This approach:
- Reuses existing sync code paths (no new migration infrastructure)
- Handles user customizations naturally via CRDT semantics
- Enables efficient delta sync on app updates (only new schema since last sync)
- Works offline (seed is bundled with the app)

### Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                              BUILD TIME                                      │
│                                                                              │
│   DDL Statements ──▶ SyncManager ──▶ Serialize ──▶ schema-seed.json         │
│   (CREATE TABLE...)   (in-memory)     Metadata                               │
│                                                                              │
│   • Fixed APP_PROVIDER_SITE_ID (well-known, e.g., all zeros)                │
│   • Build timestamp as HLC base                                              │
│   • Records: SchemaMigrations, ColumnVersions for table columns              │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│                              RUNTIME                                         │
│                                                                              │
│   ┌──────────────┐    getChangesSince()     ┌──────────────────────────┐    │
│   │  Seed Store  │ ─────────────────────▶   │   User's SyncManager     │    │
│   │  (read-only) │                          │                          │    │
│   └──────────────┘                          │   applyChanges()         │    │
│         │                                   │         │                │    │
│         │ lastSeedHLC                       │         ▼                │    │
│         │ (user metadata)                   │   Schema DDL executed    │    │
│         ▼                                   │   CRDT metadata recorded │    │
│   Only changes after lastSeedHLC            └──────────────────────────┘    │
│   are returned (efficient delta sync)                                        │
└─────────────────────────────────────────────────────────────────────────────┘
```

### The Well-Known App Provider Site ID

Use a deterministic, well-known site ID for the app provider:

```typescript
/** All-zeros site ID for app provider schema seeds */
const APP_PROVIDER_SITE_ID = new Uint8Array(16); // 16 bytes of 0x00

/** Or use a fixed base64 */
const APP_PROVIDER_SITE_ID = siteIdFromBase64('AAAAAAAAAAAAAAAAAAAAAA');
```

This ensures:
- The app provider's site ID is consistent across builds
- User's local changes (with random site IDs) won't conflict with seed schema
- Easy to identify seed-originated changes in debugging

### Efficient Delta Sync

The change log in the seed enables efficient delta sync:

1. **First launch**: `lastSeedHLC` is undefined, all seed entries are applied
2. **App update**: `lastSeedHLC` points to previous seed's latest HLC
3. **Query**: Filter change log entries where `hlc > lastSeedHLC`
4. **Result**: Only new schema changes are processed (O(k) where k = new changes)

### User Schema Customizations

Because the sync uses standard CRDT semantics, user schema customizations are handled naturally:

1. **User adds a column**: User's column has their site ID with later HLC, preserved
2. **App adds same column in update**: LWW resolves (later HLC wins, or user's if concurrent)
3. **User drops a table**: "Most destructive wins" - drop persists even if app's seed has the table

This means:
- App schema is the baseline
- User customizations layer on top
- Conflicts resolve deterministically

### What's Provided by Quereus

All the primitives needed for schema seeds are available in Quereus packages:

| Component | Package | Status |
|-----------|---------|--------|
| `SyncManager.applyChanges()` | `quereus-plugin-sync` | ✅ Available |
| `SyncManager.getPeerSyncState()` | `quereus-plugin-sync` | ✅ Available |
| `SyncManager.updatePeerSyncState()` | `quereus-plugin-sync` | ✅ Available |
| `compareHLC()`, `hlcToJson()`, `hlcFromJson()` | `quereus-plugin-sync` | ✅ Available |
| `SerializedHLC` type | `quereus-plugin-sync` | ✅ Available |
| `InMemoryKVStore` | `quereus-store` | ✅ Available |
| `siteIdToBase64()`, `siteIdFromBase64()` | `quereus-plugin-sync` | ✅ Available |
| `toBase64Url()`, `fromBase64Url()` | `quereus-plugin-sync` | ✅ Available |

**Usage:**
```typescript
import {
  SyncManager, compareHLC,
  hlcToJson, hlcFromJson, type SerializedHLC,
  siteIdToBase64, siteIdFromBase64,
  toBase64Url, fromBase64Url
} from 'quereus-plugin-sync';
import { InMemoryKVStore } from '@quereus/store';
```

### What's App-Specific

The following should be implemented in your application:

1. **`SchemaSeed` interface**: Define the JSON structure for your seed files
2. **`generateSchemaSeed()`**: Build-time script to create seeds from DDL
3. **`syncFromSchemaSeed()`**: Runtime function to apply seeds

These are app-specific because:
- Seed format may vary (JSON, MessagePack, etc.)
- Generation may integrate with your build system (Vite, webpack, etc.)
- Application may have custom sync logic or validation


