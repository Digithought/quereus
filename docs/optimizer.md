# Query Optimizer Design and Roadmap

## Overview

The query optimizer in SQLiter is responsible for choosing an efficient execution plan for SQL queries, particularly focusing on join order selection and leveraging virtual table capabilities. The core logic resides in the `src/compiler/planner/` directory, primarily within the `QueryPlannerContext` class and related helpers.

Its main responsibilities are:

1.  **Base Relation Planning:** Analyzing potential access paths for each individual base table (or subquery/function source) using the `xBestIndex` method of the corresponding virtual table module. This determines the cost and estimated rows for scanning the table under given constraints.
2.  **Join Candidate Identification:** Extracting potential join operations (INNER, LEFT, CROSS, etc.) and their conditions from the query's `FROM` clause.
3.  **Cost-Based Join Order Selection:** Employing a greedy algorithm (`planExecution` method in `QueryPlannerContext`) to iteratively select the most cost-effective join to perform next. Currently, it uses a Nested Loop Join (NLJ) cost model:
    *   It estimates the cost of executing a join with either relation as the outer loop.
    *   The cost considers the cost of scanning the outer relation plus the cost of scanning the inner relation for each row of the outer relation (`OuterCost + OuterRows * InnerCost`).
    *   Inner loop scanning cost is determined by invoking `planTableAccess` (which calls `xBestIndex`) considering constraints from the join condition potentially involving columns from the outer relation.
4.  **Execution Plan Generation:** Producing an ordered list of `PlannedStep` objects (`PlannedScanStep`, `PlannedJoinStep`) representing the chosen execution strategy.
5.  **Order By Consumption Tracking:** Analyzing the plan generated by `xBestIndex` for base scans and propagating whether the required `ORDER BY` clause is satisfied by the plan, avoiding unnecessary external sorts. NLJ inherently preserves the order of the *outer* relation stream.

## Current Planner Details (`QueryPlannerContext`)

The planning process for a `SELECT` statement generally follows these steps:

1.  **Initialization:**
    *   The `QueryPlannerContext` is created.
    *   Base relations (tables, subqueries, TVFs) mentioned in the `FROM` clause are identified.
    *   For each base relation, `planTableAccess` is called to determine the optimal scan plan using the virtual table's `xBestIndex`. This provides initial cost and row estimates. These become initial `PlannedScanStep`s.
2.  **Join Candidate Extraction:** The planner analyzes the `FROM` clause structure (including explicit `JOIN` keywords and comma-separated tables) to create a list of `JoinCandidateInfo` objects, representing potential joins between relations. Basic selectivity estimation is performed.
3.  **Greedy Join Selection Loop:**
    *   The planner iterates as long as there are multiple relations yet to be joined.
    *   In each iteration, it evaluates the cost of all possible joins between the currently available relations using the `_costJoinCandidate` method.
    *   `_costJoinCandidate` estimates the cost for both possible outer-inner loop configurations for an NLJ. It relies on calling `planTableAccess` to determine the cost of accessing the inner table based on the join condition and the outer relation's context.
    *   The join candidate with the lowest estimated NLJ cost is selected.
    *   A `PlannedJoinStep` is created, referencing the previously generated steps for its left and right inputs and specifying which input acts as the outer and inner loop based on the cost analysis.
    *   The newly created `PlannedJoinStep` replaces its input steps in the set of available relations for the next iteration.
4.  **Final Plan:** The loop continues until only one relation (representing the final result of all joins) remains, or no more joins can be performed. The sequence of `PlannedStep` objects created constitutes the execution plan.
5.  **VDBE Generation:** The `compilePlannedStepsLoop` function then takes this `PlannedStep[]` array and generates the corresponding VDBE bytecode, constructing the nested loops and condition checks according to the plan.

## Order By Optimization

The planner integrates with the `ORDER BY` clause:

*   When planning base table access (`planTableAccessHelper` calling `xBestIndex`), it checks if the VTab reports that the chosen access path already satisfies the query's `ORDER BY` clause (`orderByConsumed`).
*   This boolean result is stored in the `PlannedScanStep`.
*   The `compileSelectStatement` function checks the final `PlannedStep`. If it's a `PlannedScanStep`, it uses the `orderByConsumed` flag directly. If it's a `PlannedJoinStep`, it currently assumes NLJ preserves the outer loop's order and checks the order consumption of the step that produced the final outer relation.
*   If the order is consumed by the plan, the final external `Opcode.Sort` is skipped.

## Limitations

*   **Cost Model:** The current cost model is simple and only considers NLJ. It doesn't account for other potential join algorithms (Hash Join, Merge Join) which might be more efficient in certain scenarios. CPU costs vs. I/O costs are not explicitly modeled.
*   **Selectivity Estimation:** Join selectivity estimation is currently very basic (defaults or simple heuristics) and doesn't deeply analyze predicate specifics.
*   **Statistics:** The planner relies heavily on `estimatedRows` and `estimatedCost` returned by `xBestIndex`. The accuracy of these statistics is crucial. Currently, there's no built-in mechanism like `ANALYZE` to gather accurate statistics for base tables; modules might provide estimates, or defaults are used.
*   **Complex Predicates:** Handling of complex `WHERE` or `JOIN ON` conditions involving functions or OR clauses during costing and `xBestIndex` interaction could be improved. Predicate pushdown analysis currently checks dependencies within `AND` clauses but doesn't handle partial pushdown or `OR` clauses.
*   **Subquery/CTE Planning:** Planning for queries involving complex subqueries or CTEs within the main query block is not deeply integrated into the cost-based join ordering yet.
*   **Join Result Schema:** The schema generated for the result of a join operation is currently a placeholder (using the outer relation's schema). This needs proper implementation to accurately reflect the combined columns for subsequent planning steps or expression compilation.
*   **Intermediate LEFT JOIN Padding:** While padding for the final output of a LEFT JOIN is handled, the propagation of NULLs for intermediate LEFT JOINs (whose results feed into further joins) might be incomplete in the current VDBE generation.

## Future Work & Potential Enhancements

Given the focus on virtual tables, especially potentially distributed ones, optimizations that reduce data transfer and computation at the source are likely to yield the most significant benefits. Key priorities include:

*   **Constant Folding:** Implement thorough constant folding during compilation to simplify expressions evaluated repeatedly.
*   **Advanced Predicate Pushdown:** Enhance the interaction between the planner (`extractConstraints`) and `xBestIndex` to:
    *   Push down more complex predicates involving functions or `OR` clauses where supported by the VTable.
    *   Handle partial predicate pushdown (pushing parts of an `AND` clause even if other parts cannot be handled by the VTable).
    *   Improve analysis of predicates involving multiple tables within join conditions for more effective filtering.
*   **Subquery Optimizations:** Implement techniques like subquery flattening (converting subqueries into joins) and decorrelation (rewriting correlated subqueries to be independent) to avoid redundant computations, particularly important for distributed systems.
*   **Implement Hash Join/Merge Join:** Add costing logic and VDBE code generation for Hash Joins (useful for equi-joins on large datasets) and potentially Merge Joins (if inputs are sorted). While potentially less critical than logical optimizations given effective VTable caching, these offer alternative join strategies.
*   **Improve Cost Model:** Refine cost estimations to better reflect potential CPU vs. I/O costs, especially relevant for complex expressions or functions. Incorporate network latency considerations for distributed VTabs.
*   **Improve Selectivity Estimation:** Use histograms or more sophisticated analysis of predicates to get better estimates for join selectivity and filter effectiveness.
*   **Implement `ANALYZE`:** Add an `ANALYZE` command to gather statistics (row counts, value distributions) for tables (especially the built-in `MemoryTable`) and store them for the planner's use. Allow VTabs to optionally provide richer statistics.
*   **Enhanced Subquery/CTE Integration (Costing):** Integrate the planning of materialized CTEs and subqueries more tightly into the main query's cost-based planning *after* initial optimization techniques are applied.
*   **Pushdown Optimizations (Other):** Explore opportunities to push down projections or partial aggregations where VTabs support them.
*   **Explore Alternative Planners:** Investigate more advanced planning algorithms beyond the simple greedy approach if needed for very complex queries.

## Current Status

A functional, cost-based query planner exists, focusing on Nested Loop Join (NLJ) order selection. It interacts with the virtual table interface (`xBestIndex`) to estimate costs for base table access and inner join loop access. It successfully generates `PlannedStep` arrays that dictate the execution flow, including optimizations for consumed `ORDER BY` clauses to avoid unnecessary sorting.


## Runtime Optimization Plan

Below is a "shopping list" of runtime-level optimisations that make a measurable difference on small JS/TS interpreters such as SQLiter's VDBE.  None of them change external behaviour; they just make the same byte-code run faster or allocate less.  They are grouped roughly from "quick wins" to "larger projects" and each comes with a short implementation plan so you can decide which ones are worth the effort.

────────────────────────────────────────────────────────────────────────
1. Tighten the main dispatch loop
────────────────────────────────────────────────────────────────────────
Potential improvements for the main `VdbeRuntime.run()` loop:

- **PC Handling:** The current implementation correctly uses direct array access (`code[currentPc]`) and manages `this.pc` updates, avoiding O(n²) behavior.
- **Handler Lookup:** The handler lookup (`handlers[inst.opcode]`) is performed efficiently before checking for promises.

*Suggestion:* While the current loop is reasonably efficient, further micro-optimizations might be possible by minimizing work inside the loop, though the current structure with async handling is clear.

────────────────────────────────────────────────────────────────────────
2. Represent the stack in a cache-friendly way
────────────────────────────────────────────────────────────────────────
*Current:* Each stack cell is an object: `{ value: SqlValue }`. This adds indirection.

*Suggestion:* Keep `stack: SqlValue[]`, treat `null` as "empty". If metadata (e.g., type tags) is needed later, use a *parallel* typed array (`Uint8Array`) instead of enlarging each element object. This reduces object allocation and improves cache locality.

*Side-effect:* smaller GC churn and potentially ~10–15 % speed-up on tight loops.

────────────────────────────────────────────────────────────────────────
3. Promise/async fast-path split
────────────────────────────────────────────────────────────────────────
*Current:* Only a handful of opcodes are inherently async (VTab interactions), yet every iteration checks `result instanceof Promise`.

*Suggestion:* Split the interpreter into two loops: `runSync()` and `runAsync()`. `runSync()` executes until it hits an async opcode, then transitions to `runAsync()` which uses `await`. This removes the promise check overhead for purely synchronous statements.

*Expected gain:* ~6-7 % overhead reduction from the main loop for sync-only operations.

────────────────────────────────────────────────────────────────────────
4. Mini peephole optimiser (**Compile-Time**)
────────────────────────────────────────────────────────────────────────
*Suggestion:* Implement a simple peephole optimizer that runs *after* compilation but *before* runtime. Even 4–5 simple rules can yield significant wins:

*Rule examples:*
a) `SCopy A B` immediately followed by any write to `B` ⇒ drop the copy.  
b) `Null A; IsNull A B`  ⇒ replace pair with `Goto B`.  
c) `Move A B` where `A == B` ⇒ delete.  
d) Back-to-back `Goto` chains ⇒ collapse to final target.  
e) Constant folding of `Function` when `nArgs=0` and pure (e.g. `random()`).

*Plan:*
1. Add `optimizeProgram(program: VdbeProgram)` called by the `Compiler`.
2. Run a sliding window (2–3 instructions) over `program.instructions`; rewrite the array.
3. Keep address-adjust fix-ups in a map (`old addr ➔ new addr`) to patch jumps.

*Expected gain:* Trims 5–20 % of dynamic instructions executed.

────────────────────────────────────────────────────────────────────────
5. Specialised "super-instructions" (**Compile-Time/ISA**)
────────────────────────────────────────────────────────────────────────
*Suggestion:* Identify common, hot patterns of multiple instructions (e.g., `Column → Affinity → IsNull → IfTrue <jump>`) and replace them with a single, specialized opcode during compilation or peephole optimization. Add a corresponding handler for the new opcode to the runtime.

*Expected gain:* Two or three such "super-ops" can cover a large percentage of executed code paths, reducing dispatch overhead.

────────────────────────────────────────────────────────────────────────
6. Inline common UDFs and aggregates (**Compile-Time**)
────────────────────────────────────────────────────────────────────────
*Suggestion:* For common built-in functions (`length()`, `upper()`, `count(*)`, etc.), perform static analysis during compilation:

• If arguments are literals, evaluate at compile time.
• If arguments are simple register reads, translate to a dedicated opcode (e.g., `StrLen`, `ToUpper`, `CountStep`) instead of the generic `Function` opcode.

*Benefit:* Removes `FunctionContext` allocation and dynamic dispatch overhead for these common cases.

────────────────────────────────────────────────────────────────────────
7. Cursor & virtual-table caching
────────────────────────────────────────────────────────────────────────
*Current:* Virtual table `filter()` and `next()` likely allocate new row data frequently.

*Suggestion:* Enhance `VdbeCursor` and the VTab interface. Let `VdbeCursor` keep a small reusable row buffer (`SqlValue[] currentRow`). Module implementers could write into that shared buffer instead of allocating new arrays/objects per row. Provide helpers in `VirtualTableModule` (e.g., `beginRow(writeFn)`).

*Benefit:* Reduces allocation and GC pressure during data iteration.

────────────────────────────────────────────────────────────────────────
8. Adaptive stack growth strategy
────────────────────────────────────────────────────────────────────────
*Current:* `setStack` doubles the stack size (`newLength = Math.max(this.stack.length * 2, index + 1)`) when the requested index is out of bounds.

*Suggestion:* For very large stack requirements (e.g., `INSERT … SELECT` with many rows/columns), doubling can lead to large, infrequent allocations. Consider switching to fixed increments past a certain threshold:

```ts
if (index >= stack.length) {
    const increment = stack.length > 1024 ? 1024 : stack.length; // Example threshold
    stack.length = Math.max(index + 1, stack.length + increment);
}
```

*Benefit:* Can reduce pathological GC pauses with extremely large stack usage.

────────────────────────────────────────────────────────────────────────
9. Optional JIT (**Future/Research**)
────────────────────────────────────────────────────────────────────────
*Suggestion:* Emit plain JavaScript code strings corresponding to the VDBE instruction block and execute it using `new Function()`.

```js
// VDBE: add R3,R4,R5
// JIT output: code += 'R[5] = R[3] + R[4];';
```

*Benefit:* Allows the underlying JavaScript engine (V8, etc.) to apply advanced optimizations. Suitable for heavy analytic/ETL workloads where compilation overhead is acceptable.

────────────────────────────────────────────────────────────────────────
10. Instrumentation & regression benchmarks (**Future/Diagnostics**)
────────────────────────────────────────────────────────────────────────
*Suggestion:* Add a lightweight timer/counter struct to `VmCtx` guarded by a build or runtime flag. Report metrics like:

• # executed opcodes
• # stack reallocations
• # Promise awaits
• Time per opcode class

Use a representative workload suite to catch performance regressions automatically.

────────────────────────────────────────────────────────────────────────
Summary roadmap
────────────────────────────────────────────────────────────────────────
*(Adjusted based on current implementation)*
Phase 1 (quick wins, 1–2 days)
• Stack object → array (2)

Phase 2 (1 week)
• Async fast-path split (3)
• Initial peephole rules (compile-time) (4)
• Adaptive stack growth (8)

Phase 3 (2–3 weeks)
• Super-instructions (compile-time/ISA) (5)
• Built-in UDF inlining (compile-time) (6)
• Cursor row buffering (7)

Phase 4 (research)
• JS-JIT backend (9)
• Continuous perf harness (10)

Each phase is independent and delivers value on its own, so you can stop
once the effort/benefit ratio no longer favours additional work.
